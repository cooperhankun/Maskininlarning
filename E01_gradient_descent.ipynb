{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/kokchun/Maskininlarning-AI21/blob/main/Exercises/E01_gradient_descent.ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; to see hints and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient descent exercises\n",
    "\n",
    "---\n",
    "These are introductory exercises in Machine learning with focus in **gradient descent** .\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> all datasets used in this exercise can be found under Data folder of the course Github repo</p>\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that in cases when you start to repeat code, try not to. Create functions to reuse code instead. </p>\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Remember</b> to use <b>descriptive variable, function, index </b> and <b> column names</b> in order to get readable code </p>\n",
    "\n",
    "The number of stars (\\*), (\\*\\*), (\\*\\*\\*) denotes the difficulty level of the task\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Simulate dataset (*)\n",
    "\n",
    "Simulate datasets according to these rules:\n",
    "\n",
    "- set random seed to 42\n",
    "- (1000,2) samples from $X \\sim \\mathcal{U}(0,1)$ , i.e. 1000 rows, 2 columns. \n",
    "- 1000 samples from $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "- $y = 3x_1 + 5x_2 + 3 + \\epsilon$ , where $x_i$ is column $i$ of $X$\n",
    "\n",
    "Finally add a column of ones for the intercept to $X$.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use for simulating X\n",
    "\n",
    "´´´\n",
    "np.random.rand(samples, 2)\n",
    "´´´\n",
    "\n",
    "to concatenate with ones, use ```np.c_[..., ...]```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "```\n",
    "array([[1.        , 0.37454012, 0.95071431],\n",
    "       [1.        , 0.73199394, 0.59865848],\n",
    "       [1.        , 0.15601864, 0.15599452],\n",
    "       [1.        , 0.05808361, 0.86617615],\n",
    "       [1.        , 0.60111501, 0.70807258]])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.37454012, 0.95071431],\n",
       "       [1.        , 0.73199394, 0.59865848],\n",
       "       [1.        , 0.15601864, 0.15599452],\n",
       "       ...,\n",
       "       [1.        , 0.75137509, 0.65695516],\n",
       "       [1.        , 0.95661462, 0.06895802],\n",
       "       [1.        , 0.05705472, 0.28218707]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "np.random.seed(42)\n",
    "samples = 1000\n",
    "\n",
    "X = np.random.rand(samples, 2)\n",
    "X_1 = X[:,0]\n",
    "X_2 = X[:,1]\n",
    "e = np.random.randn(samples, 1)\n",
    "y = 3*X_1 + 5*X_2 + 3 + e\n",
    "\n",
    "X = np.c_[np.ones(samples), X]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37454012, 0.73199394, 0.15601864, 0.05808361, 0.60111501,\n",
       "       0.02058449, 0.83244264, 0.18182497, 0.30424224, 0.43194502,\n",
       "       0.61185289, 0.29214465, 0.45606998, 0.19967378, 0.59241457,\n",
       "       0.60754485, 0.06505159, 0.96563203, 0.30461377, 0.68423303,\n",
       "       0.12203823, 0.03438852, 0.25877998, 0.31171108, 0.54671028,\n",
       "       0.96958463, 0.93949894, 0.59789998, 0.0884925 , 0.04522729,\n",
       "       0.38867729, 0.82873751, 0.28093451, 0.14092422, 0.07455064,\n",
       "       0.77224477, 0.00552212, 0.70685734, 0.77127035, 0.35846573,\n",
       "       0.86310343, 0.33089802, 0.31098232, 0.72960618, 0.88721274,\n",
       "       0.11959425, 0.76078505, 0.77096718, 0.52273283, 0.02541913,\n",
       "       0.03142919, 0.31435598, 0.90756647, 0.41038292, 0.22879817,\n",
       "       0.28975145, 0.92969765, 0.63340376, 0.80367208, 0.892559  ,\n",
       "       0.80744016, 0.31800347, 0.22793516, 0.81801477, 0.00695213,\n",
       "       0.417411  , 0.11986537, 0.9429097 , 0.51879062, 0.3636296 ,\n",
       "       0.96244729, 0.49724851, 0.28484049, 0.60956433, 0.05147875,\n",
       "       0.90826589, 0.14489487, 0.98565045, 0.67213555, 0.23763754,\n",
       "       0.36778313, 0.63352971, 0.09028977, 0.32078006, 0.04077514,\n",
       "       0.67756436, 0.51209306, 0.64517279, 0.69093774, 0.93672999,\n",
       "       0.34106635, 0.92469362, 0.25794163, 0.8172222 , 0.52965058,\n",
       "       0.09310277, 0.90041806, 0.33902979, 0.72595568, 0.88708642,\n",
       "       0.64203165, 0.16162871, 0.60642906, 0.10147154, 0.00506158,\n",
       "       0.54873379, 0.65196126, 0.71217922, 0.3253997 , 0.6496329 ,\n",
       "       0.65761289, 0.09367477, 0.26520237, 0.97301055, 0.89204656,\n",
       "       0.7948113 , 0.57690388, 0.19524299, 0.28077236, 0.6454723 ,\n",
       "       0.94045858, 0.91486439, 0.01545662, 0.42818415, 0.96361998,\n",
       "       0.29444889, 0.85113667, 0.16949275, 0.93615477, 0.57006117,\n",
       "       0.61500723, 0.14008402, 0.87737307, 0.69701574, 0.35949115,\n",
       "       0.80936116, 0.86707232, 0.5113424 , 0.79829518, 0.70196688,\n",
       "       0.89000534, 0.37558295, 0.57828014, 0.46559802, 0.28654125,\n",
       "       0.03050025, 0.82260056, 0.12706051, 0.76999355, 0.62289048,\n",
       "       0.05168172, 0.54063512, 0.72609133, 0.51630035, 0.79518619,\n",
       "       0.43897142, 0.02535074, 0.83598012, 0.40895294, 0.15643704,\n",
       "       0.54922666, 0.66019738, 0.95486528, 0.55435405, 0.41960006,\n",
       "       0.35597268, 0.01439349, 0.04600264, 0.85546058, 0.47417383,\n",
       "       0.49161588, 0.17320187, 0.39850473, 0.63509365, 0.37461261,\n",
       "       0.50313626, 0.65869363, 0.07056875, 0.02651131, 0.94023024,\n",
       "       0.38816993, 0.45825289, 0.94146481, 0.96119056, 0.19579113,\n",
       "       0.100778  , 0.09444296, 0.07118865, 0.84487531, 0.81446848,\n",
       "       0.11816483, 0.62894285, 0.73507104, 0.28203457, 0.75061475,\n",
       "       0.99050514, 0.37201809, 0.34080354, 0.85841275, 0.75087107,\n",
       "       0.10312387, 0.50525237, 0.3200496 , 0.38920168, 0.90538198,\n",
       "       0.31931364, 0.95060715, 0.63183721, 0.29321077, 0.67251846,\n",
       "       0.79157904, 0.0912061 , 0.05755876, 0.4415305 , 0.35091501,\n",
       "       0.14299168, 0.61821806, 0.08410681, 0.07276301, 0.70624223,\n",
       "       0.08483771, 0.3742708 , 0.81279957, 0.98600106, 0.37625959,\n",
       "       0.77714692, 0.42422201, 0.11119748, 0.01135364, 0.05630328,\n",
       "       0.11752625, 0.74604488, 0.96217255, 0.28571209, 0.22359584,\n",
       "       0.01215447, 0.04315991, 0.52770111, 0.07379656, 0.96930254,\n",
       "       0.62939864, 0.45454106, 0.58431431, 0.04544638, 0.95041148,\n",
       "       0.45565675, 0.27738118, 0.4636984 , 0.58365611, 0.97439481,\n",
       "       0.69816171, 0.30952762, 0.68473117, 0.91092718, 0.94979991,\n",
       "       0.6134152 , 0.93272848, 0.04521867, 0.37646337, 0.98727613,\n",
       "       0.59413072, 0.9699144 , 0.8383287 , 0.4148195 , 0.0563755 ,\n",
       "       0.81290101, 0.99663684, 0.76898742, 0.84964739, 0.45054414,\n",
       "       0.95405103, 0.22864281, 0.61812824, 0.11355759, 0.5203077 ,\n",
       "       0.5201635 , 0.55190684, 0.8766536 , 0.13401523, 0.75513726,\n",
       "       0.70407977, 0.13637148, 0.35058756, 0.39224405, 0.90415869,\n",
       "       0.51398949, 0.39654278, 0.86236371, 0.14707348, 0.49211629,\n",
       "       0.45913576, 0.49261809, 0.63340085, 0.07586333, 0.12804584,\n",
       "       0.13882717, 0.18188008, 0.89678841, 0.66755774, 0.19228902,\n",
       "       0.16893506, 0.17701048, 0.12063587, 0.20633372, 0.50341727,\n",
       "       0.03931214, 0.62790039, 0.87357862, 0.06107796, 0.80620128,\n",
       "       0.18452102, 0.3704721 , 0.61825477, 0.46253472, 0.0366832 ,\n",
       "       0.71334959, 0.51167744, 0.10717201, 0.53261727, 0.26924323,\n",
       "       0.0200712 , 0.21144801, 0.11976213, 0.59359245, 0.78917124,\n",
       "       0.08692029, 0.58684112, 0.43165955, 0.28377591, 0.64591724,\n",
       "       0.35609673, 0.60577482, 0.10178247, 0.24595773, 0.18656702,\n",
       "       0.1733736 , 0.08023375, 0.41039683, 0.1120389 , 0.96947043,\n",
       "       0.81707207, 0.17088759, 0.92937599, 0.57161269, 0.76949293,\n",
       "       0.32367924, 0.50761038, 0.11483682, 0.28863055, 0.15436272,\n",
       "       0.53258943, 0.33660428, 0.06337497, 0.32235384, 0.25464065,\n",
       "       0.76022786, 0.47157619, 0.34886827, 0.83061941, 0.12429722,\n",
       "       0.93834046, 0.06649627, 0.57447311, 0.13977238, 0.20162732,\n",
       "       0.1642658 , 0.66519722, 0.35883048, 0.39244511, 0.43913491,\n",
       "       0.46267979, 0.74760938, 0.2322127 , 0.38389122, 0.90647211,\n",
       "       0.11689804, 0.62770805, 0.13927207, 0.62007276, 0.89389258,\n",
       "       0.15167488, 0.24848914, 0.03353243, 0.76245869, 0.34208175,\n",
       "       0.11063174, 0.12748866, 0.79729537, 0.2292514 , 0.72003654,\n",
       "       0.69394844, 0.25179906, 0.18159772, 0.58339179, 0.4620058 ,\n",
       "       0.1533514 , 0.50588868, 0.01811018, 0.93211828, 0.69665082,\n",
       "       0.70723863, 0.57628836, 0.42413067, 0.93436701, 0.45083937,\n",
       "       0.9848412 , 0.12466268, 0.86989636, 0.59127544, 0.05476164,\n",
       "       0.80285345, 0.33349917, 0.5373956 , 0.34634599, 0.73750125,\n",
       "       0.22460482, 0.14085702, 0.49836777, 0.9148459 , 0.58058835,\n",
       "       0.01309446, 0.17803597, 0.14866273, 0.08534967, 0.50219501,\n",
       "       0.06707648, 0.20990559, 0.20513964, 0.03654967, 0.56484113,\n",
       "       0.77552762, 0.52439027, 0.40076306, 0.15524025, 0.86178562,\n",
       "       0.37330932, 0.64399954, 0.02538636, 0.71597223, 0.02709599,\n",
       "       0.2310748 , 0.01971054, 0.79991609, 0.65274611, 0.09944139,\n",
       "       0.72226693, 0.83021986, 0.66808514, 0.29314773, 0.01300192,\n",
       "       0.20788626, 0.18143544, 0.42142455, 0.81744356, 0.25942343,\n",
       "       0.59029494, 0.62414891, 0.55204718, 0.29446576, 0.76360579,\n",
       "       0.86846798, 0.89455223, 0.4252135 , 0.26867736, 0.63347822,\n",
       "       0.13935607, 0.98440218, 0.17167929, 0.01839068, 0.11775108,\n",
       "       0.27405522, 0.65142039, 0.20642127, 0.13688563, 0.87389008,\n",
       "       0.60051686, 0.17537128, 0.41877052, 0.51891771, 0.16628337,\n",
       "       0.08279867, 0.24534911, 0.28869374, 0.71904591, 0.56640464,\n",
       "       0.66367117, 0.7325721 , 0.03118314, 0.59507793, 0.49636625,\n",
       "       0.33424389, 0.10659825, 0.72818876, 0.6884024 , 0.24640203,\n",
       "       0.79941588, 0.27214514, 0.3609739 , 0.91731358, 0.95023735,\n",
       "       0.18513293, 0.87294584, 0.80656115, 0.69227656, 0.24966801,\n",
       "       0.22120944, 0.94405934, 0.70557517, 0.18057535, 0.9154883 ,\n",
       "       0.69742027, 0.9243962 , 0.94426649, 0.86204265, 0.31910047,\n",
       "       0.03700763, 0.23000884, 0.0769532 , 0.33987496, 0.06535634,\n",
       "       0.53949129, 0.3187525 , 0.88597775, 0.23295947, 0.87009887,\n",
       "       0.87470167, 0.9390677 , 0.99793411, 0.76718829, 0.47987562,\n",
       "       0.87367711, 0.76827341, 0.421357  , 0.23877715, 0.35462216,\n",
       "       0.29630812, 0.04209319, 0.98772239, 0.38432665, 0.21825389,\n",
       "       0.78634501, 0.41758078, 0.94473202, 0.61341139, 0.99116863,\n",
       "       0.94273177, 0.60773679, 0.23066981, 0.22048621, 0.77958447,\n",
       "       0.05784268, 0.88378588, 0.99490782, 0.39624202, 0.69602062,\n",
       "       0.81583312, 0.22381761, 0.59293993, 0.09148684, 0.26560004,\n",
       "       0.88874808, 0.86212762, 0.65524198, 0.08698676, 0.37268852,\n",
       "       0.72342011, 0.08104622, 0.68325876, 0.85120691, 0.48058658,\n",
       "       0.82468097, 0.67801615, 0.26702827, 0.79742602, 0.85058173,\n",
       "       0.70836298, 0.69747146, 0.61861138, 0.15860511, 0.87184353,\n",
       "       0.82581675, 0.33511885, 0.1607599 , 0.83213418, 0.00638587,\n",
       "       0.61692692, 0.63181353, 0.6340057 , 0.7798454 , 0.7610279 ,\n",
       "       0.962992  , 0.63262189, 0.10250973, 0.68788572, 0.30096357,\n",
       "       0.0673506 , 0.34588306, 0.04574203, 0.97348897, 0.74965183,\n",
       "       0.7582632 , 0.02212355, 0.48864319, 0.68329538, 0.27362667,\n",
       "       0.4261813 , 0.16362382, 0.69368223, 0.08238105, 0.65451121,\n",
       "       0.95086356, 0.4323348 , 0.41972732, 0.3975944 , 0.98397765,\n",
       "       0.8940992 , 0.2131047 , 0.65166683, 0.86435825, 0.96819343,\n",
       "       0.86862317, 0.77092184, 0.76102399, 0.13124488, 0.92084785,\n",
       "       0.79653729, 0.11730819, 0.68556529, 0.20052473, 0.06420894,\n",
       "       0.2689934 , 0.31036196, 0.01162054, 0.39249356, 0.60002055,\n",
       "       0.69498189, 0.77985099, 0.48050695, 0.24204502, 0.14249554,\n",
       "       0.61815573, 0.55964868, 0.32646131, 0.0878665 , 0.03320311,\n",
       "       0.39692328, 0.56754085, 0.8005867 , 0.16748258, 0.63643025,\n",
       "       0.03158614, 0.05197128, 0.70906052, 0.71408693, 0.33945019,\n",
       "       0.08011485, 0.54759238, 0.45231828, 0.52640266, 0.08162998,\n",
       "       0.24710323, 0.87178357, 0.97586526, 0.18211792, 0.65870778,\n",
       "       0.55536355, 0.22845474, 0.97479316, 0.19954245, 0.07219841,\n",
       "       0.25768289, 0.86827251, 0.74270652, 0.34593499, 0.98764956,\n",
       "       0.8670315 , 0.43861542, 0.48666894, 0.90070186, 0.2768278 ,\n",
       "       0.91236335, 0.62296658, 0.73311302, 0.71582496, 0.17968311,\n",
       "       0.97139509, 0.85438509, 0.24723107, 0.44530526, 0.35923337,\n",
       "       0.16352387, 0.96941232, 0.65673666, 0.77347313, 0.96982105,\n",
       "       0.23605046, 0.16975791, 0.33700318, 0.43088752, 0.61714499,\n",
       "       0.16704191, 0.03667143, 0.66380453, 0.84417045, 0.58535436,\n",
       "       0.20584121, 0.26974961, 0.53116953, 0.03934354, 0.45219903,\n",
       "       0.3161561 , 0.04157286, 0.98663012, 0.00493998, 0.63911994,\n",
       "       0.45473986, 0.48884658, 0.13965125, 0.30792994, 0.20185345,\n",
       "       0.96991205, 0.67260212, 0.86814225, 0.69262595, 0.94461422,\n",
       "       0.49717476, 0.86890498, 0.03038706, 0.68952675, 0.21567515,\n",
       "       0.39386441, 0.10659303, 0.99941373, 0.97717418, 0.87075345,\n",
       "       0.56701626, 0.87851556, 0.32703316, 0.80784594, 0.79781365,\n",
       "       0.81783422, 0.5444891 , 0.32458583, 0.39617269, 0.3885581 ,\n",
       "       0.23754413, 0.22726963, 0.60344859, 0.61949035, 0.37978578,\n",
       "       0.51908179, 0.02564207, 0.38019562, 0.58017237, 0.60790509,\n",
       "       0.81298574, 0.9555237 , 0.19577799, 0.64747471, 0.2434823 ,\n",
       "       0.06026739, 0.35162269, 0.48587176, 0.2848729 , 0.8030259 ,\n",
       "       0.3113077 , 0.71615067, 0.4135491 , 0.18114935, 0.18143835,\n",
       "       0.70904626, 0.56731222, 0.96292688, 0.80599255, 0.04341253,\n",
       "       0.95140334, 0.81918886, 0.22807977, 0.61098099, 0.8398613 ,\n",
       "       0.35342138, 0.78052552, 0.82261432, 0.6675499 , 0.62385932,\n",
       "       0.58660846, 0.73687374, 0.2167398 , 0.02363859, 0.60709404,\n",
       "       0.2319471 , 0.59447634, 0.98778552, 0.69514455, 0.42819961,\n",
       "       0.69243615, 0.12839429, 0.72433882, 0.27416067, 0.08565825,\n",
       "       0.19186732, 0.2266564 , 0.06942384, 0.06761256, 0.23371208,\n",
       "       0.88007909, 0.53295779, 0.33300191, 0.99413936, 0.55778342,\n",
       "       0.46520561, 0.56229682, 0.17530294, 0.20093369, 0.09667645,\n",
       "       0.75616333, 0.66491172, 0.92717782, 0.39931592, 0.9924835 ,\n",
       "       0.53995713, 0.52095798, 0.08912443, 0.12771348, 0.78202809,\n",
       "       0.03616038, 0.26311257, 0.08764275, 0.55380224, 0.39698152,\n",
       "       0.60059433, 0.91939197, 0.99215801, 0.20851051, 0.1163664 ,\n",
       "       0.38062329, 0.86805669, 0.79003044, 0.08091928, 0.17352451,\n",
       "       0.34609973, 0.64097208, 0.13252467, 0.92275719, 0.60625294,\n",
       "       0.17483863, 0.39866303, 0.36753442, 0.02581191, 0.96311511,\n",
       "       0.96582216, 0.31181613, 0.43951169, 0.64082631, 0.61958795,\n",
       "       0.15202485, 0.78076159, 0.0581638 , 0.05778056, 0.98367893,\n",
       "       0.14224937, 0.30327515, 0.69216134, 0.50942213, 0.81397027,\n",
       "       0.30625362, 0.52704146, 0.13071038, 0.44978465, 0.36775935,\n",
       "       0.8275379 , 0.76930489, 0.416154  , 0.01919228, 0.76028982,\n",
       "       0.53531013, 0.01212077, 0.97587375, 0.95957664, 0.1097362 ,\n",
       "       0.45437733, 0.09808258, 0.15004866, 0.73735708, 0.37588829,\n",
       "       0.45944677, 0.89208469, 0.78690339, 0.68813471, 0.25467062,\n",
       "       0.03842635, 0.46147746, 0.65935392, 0.63666968, 0.06665204,\n",
       "       0.150169  , 0.51222192, 0.0416729 , 0.71632339, 0.07125673,\n",
       "       0.9565014 , 0.35325141, 0.34970323, 0.66137061, 0.17410934,\n",
       "       0.66030272, 0.26504643, 0.08217167, 0.29544478, 0.62466357,\n",
       "       0.20568726, 0.61501297, 0.64390425, 0.04195122, 0.79871419,\n",
       "       0.97997033, 0.58242266, 0.81176979, 0.12809575, 0.92808364,\n",
       "       0.37216702, 0.43940499, 0.94307584, 0.12150138, 0.8869249 ,\n",
       "       0.28590679, 0.86137   , 0.91892653, 0.75504193, 0.84199855,\n",
       "       0.77644745, 0.17742877, 0.98466974, 0.04317374, 0.13172877,\n",
       "       0.81778533, 0.50585269, 0.73280154, 0.59034769, 0.29754845,\n",
       "       0.6888853 , 0.63629135, 0.16007163, 0.00933162, 0.72646172,\n",
       "       0.0991781 , 0.80007097, 0.55508495, 0.61598545, 0.35538457,\n",
       "       0.55422651, 0.76099076, 0.74573378, 0.95807348, 0.32693162,\n",
       "       0.30560422, 0.99633434, 0.44861063, 0.88619578, 0.3915257 ,\n",
       "       0.69561815, 0.61958934, 0.79419733, 0.58820227, 0.64232553,\n",
       "       0.57998379, 0.56066009, 0.67646794, 0.26982072, 0.49825568,\n",
       "       0.05855093, 0.78489698, 0.78861496, 0.440199  , 0.32819275,\n",
       "       0.08860043, 0.59822529, 0.99834751, 0.6425652 , 0.63617736,\n",
       "       0.11833619, 0.83980229, 0.57187227, 0.18447625, 0.33451129,\n",
       "       0.02419176, 0.27307081, 0.29872557, 0.25929676, 0.87273025,\n",
       "       0.18610142, 0.45818689, 0.13347997, 0.72793931, 0.4368507 ,\n",
       "       0.7655129 , 0.61022515, 0.75137509, 0.95661462, 0.05705472])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient descent - learning rate (*)\n",
    "\n",
    "Use gradient descent to calculate $\\vec{\\theta} = (\\theta_0, \\theta_1, \\theta_2)^T$ \n",
    "\n",
    "&nbsp; a) Use $\\eta = 0.1$ and simulate 500 epochs of batch gradient descent. Plot the resulting $\\vec{\\theta}$ values for every 5th epoch. (*)\n",
    "\n",
    "&nbsp; b) Do the same as for a) but with learning rate $\\eta = 0.01$, 5000 epochs and plot every 20 step. What do you notice when changing the learning rate? (*)\n",
    "\n",
    "&nbsp; c) Experiment with larger and smaller $\\eta$ and see what happens.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use for simulating X\n",
    "\n",
    "´´´\n",
    "np.random.rand(samples, 2)\n",
    "´´´\n",
    "\n",
    "to concatenate with ones, use ```np.c_[..., ...]```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Answer</summary>\n",
    "\n",
    "a) \n",
    "\n",
    "<img src=\"../assets/grad_desc_converg.png\" height=\"200\"/>\n",
    "\n",
    "b) \n",
    "\n",
    "<img src=\"../assets/grad_desc_converg_001.png\" height=\"200\"/>\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         theta \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mgradient\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m theta\n\u001b[1;32m---> 12\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m theta\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, y, learning_rate, iterations)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m      7\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39mm\u001b[38;5;241m*\u001b[39mX\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m@\u001b[39m(X\u001b[38;5;129m@theta\u001b[39m\u001b[38;5;241m-\u001b[39my)\n\u001b[1;32m----> 8\u001b[0m     theta \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mgradient\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m theta\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,1000)"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, learning_rate = .1, iterations = 500):\n",
    "    m = len(X)\n",
    "\n",
    "    theta = np.random.randn(X.shape[1],1)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        gradient = 2/m*X.T@(X@theta-y)\n",
    "        theta -= learning_rate*gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "theta = gradient_descent(X,y)\n",
    "theta.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic Gradient Descent - learning rate (**)\n",
    "\n",
    "Repeat task 1 but using stochastic gradient descent instead. Also adjust number of epochs to see if you can find convergence. What kind of conclusions can you draw from your experiments. (**)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini Batch Gradient Descent (**)\n",
    "\n",
    "Now try different sizes of mini-batches and make some exploratory plots to see convergence. Also you can make comparison to the other algorithms by using same $\\eta$ and same amount of epochs to see how they differ from each other in terms of convergence. (**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Kokchun Giang\n",
    "\n",
    "[LinkedIn][linkedIn_kokchun]\n",
    "\n",
    "[GitHub portfolio][github_portfolio]\n",
    "\n",
    "[linkedIn_kokchun]: https://www.linkedin.com/in/kokchungiang/\n",
    "[github_portfolio]: https://github.com/kokchun/Portfolio-Kokchun-Giang\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d599e8ed57b4ba34dca595ae6c2c24f79d390e3613c4259c7061402a6e6c1dd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('Maskininlarning-oF7sSZe7': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
